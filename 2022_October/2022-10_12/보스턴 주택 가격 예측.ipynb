{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51f462de",
   "metadata": {},
   "source": [
    "## 1.목표\n",
    "- 보스턴에 있는 주택들의 데이터를 바탕으로 집 가격을 예측해보자"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669db8c0",
   "metadata": {},
   "source": [
    "## 2. 데이터 수집 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b20a3934",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b638091",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore',category=FutureWarning)\n",
    "#코드 실행시 경고가 안보이게 하는 코드(실행은 되지만 분홍색으로 나오는 오류)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca8500fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = load_boston()\n",
    "#데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cc1e01d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'feature_names', 'DESCR', 'filename', 'data_module'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston.keys()\n",
    "#키 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd29585a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([24. , 21.6, 34.7, 33.4, 36.2, 28.7, 22.9, 27.1, 16.5, 18.9, 15. ,\n",
       "       18.9, 21.7, 20.4, 18.2, 19.9, 23.1, 17.5, 20.2, 18.2, 13.6, 19.6,\n",
       "       15.2, 14.5, 15.6, 13.9, 16.6, 14.8, 18.4, 21. , 12.7, 14.5, 13.2,\n",
       "       13.1, 13.5, 18.9, 20. , 21. , 24.7, 30.8, 34.9, 26.6, 25.3, 24.7,\n",
       "       21.2, 19.3, 20. , 16.6, 14.4, 19.4, 19.7, 20.5, 25. , 23.4, 18.9,\n",
       "       35.4, 24.7, 31.6, 23.3, 19.6, 18.7, 16. , 22.2, 25. , 33. , 23.5,\n",
       "       19.4, 22. , 17.4, 20.9, 24.2, 21.7, 22.8, 23.4, 24.1, 21.4, 20. ,\n",
       "       20.8, 21.2, 20.3, 28. , 23.9, 24.8, 22.9, 23.9, 26.6, 22.5, 22.2,\n",
       "       23.6, 28.7, 22.6, 22. , 22.9, 25. , 20.6, 28.4, 21.4, 38.7, 43.8,\n",
       "       33.2, 27.5, 26.5, 18.6, 19.3, 20.1, 19.5, 19.5, 20.4, 19.8, 19.4,\n",
       "       21.7, 22.8, 18.8, 18.7, 18.5, 18.3, 21.2, 19.2, 20.4, 19.3, 22. ,\n",
       "       20.3, 20.5, 17.3, 18.8, 21.4, 15.7, 16.2, 18. , 14.3, 19.2, 19.6,\n",
       "       23. , 18.4, 15.6, 18.1, 17.4, 17.1, 13.3, 17.8, 14. , 14.4, 13.4,\n",
       "       15.6, 11.8, 13.8, 15.6, 14.6, 17.8, 15.4, 21.5, 19.6, 15.3, 19.4,\n",
       "       17. , 15.6, 13.1, 41.3, 24.3, 23.3, 27. , 50. , 50. , 50. , 22.7,\n",
       "       25. , 50. , 23.8, 23.8, 22.3, 17.4, 19.1, 23.1, 23.6, 22.6, 29.4,\n",
       "       23.2, 24.6, 29.9, 37.2, 39.8, 36.2, 37.9, 32.5, 26.4, 29.6, 50. ,\n",
       "       32. , 29.8, 34.9, 37. , 30.5, 36.4, 31.1, 29.1, 50. , 33.3, 30.3,\n",
       "       34.6, 34.9, 32.9, 24.1, 42.3, 48.5, 50. , 22.6, 24.4, 22.5, 24.4,\n",
       "       20. , 21.7, 19.3, 22.4, 28.1, 23.7, 25. , 23.3, 28.7, 21.5, 23. ,\n",
       "       26.7, 21.7, 27.5, 30.1, 44.8, 50. , 37.6, 31.6, 46.7, 31.5, 24.3,\n",
       "       31.7, 41.7, 48.3, 29. , 24. , 25.1, 31.5, 23.7, 23.3, 22. , 20.1,\n",
       "       22.2, 23.7, 17.6, 18.5, 24.3, 20.5, 24.5, 26.2, 24.4, 24.8, 29.6,\n",
       "       42.8, 21.9, 20.9, 44. , 50. , 36. , 30.1, 33.8, 43.1, 48.8, 31. ,\n",
       "       36.5, 22.8, 30.7, 50. , 43.5, 20.7, 21.1, 25.2, 24.4, 35.2, 32.4,\n",
       "       32. , 33.2, 33.1, 29.1, 35.1, 45.4, 35.4, 46. , 50. , 32.2, 22. ,\n",
       "       20.1, 23.2, 22.3, 24.8, 28.5, 37.3, 27.9, 23.9, 21.7, 28.6, 27.1,\n",
       "       20.3, 22.5, 29. , 24.8, 22. , 26.4, 33.1, 36.1, 28.4, 33.4, 28.2,\n",
       "       22.8, 20.3, 16.1, 22.1, 19.4, 21.6, 23.8, 16.2, 17.8, 19.8, 23.1,\n",
       "       21. , 23.8, 23.1, 20.4, 18.5, 25. , 24.6, 23. , 22.2, 19.3, 22.6,\n",
       "       19.8, 17.1, 19.4, 22.2, 20.7, 21.1, 19.5, 18.5, 20.6, 19. , 18.7,\n",
       "       32.7, 16.5, 23.9, 31.2, 17.5, 17.2, 23.1, 24.5, 26.6, 22.9, 24.1,\n",
       "       18.6, 30.1, 18.2, 20.6, 17.8, 21.7, 22.7, 22.6, 25. , 19.9, 20.8,\n",
       "       16.8, 21.9, 27.5, 21.9, 23.1, 50. , 50. , 50. , 50. , 50. , 13.8,\n",
       "       13.8, 15. , 13.9, 13.3, 13.1, 10.2, 10.4, 10.9, 11.3, 12.3,  8.8,\n",
       "        7.2, 10.5,  7.4, 10.2, 11.5, 15.1, 23.2,  9.7, 13.8, 12.7, 13.1,\n",
       "       12.5,  8.5,  5. ,  6.3,  5.6,  7.2, 12.1,  8.3,  8.5,  5. , 11.9,\n",
       "       27.9, 17.2, 27.5, 15. , 17.2, 17.9, 16.3,  7. ,  7.2,  7.5, 10.4,\n",
       "        8.8,  8.4, 16.7, 14.2, 20.8, 13.4, 11.7,  8.3, 10.2, 10.9, 11. ,\n",
       "        9.5, 14.5, 14.1, 16.1, 14.3, 11.7, 13.4,  9.6,  8.7,  8.4, 12.8,\n",
       "       10.5, 17.1, 18.4, 15.4, 10.8, 11.8, 14.9, 12.6, 14.1, 13. , 13.4,\n",
       "       15.2, 16.1, 17.8, 14.9, 14.1, 12.7, 13.5, 14.9, 20. , 16.4, 17.7,\n",
       "       19.5, 20.2, 21.4, 19.9, 19. , 19.1, 19.1, 20.1, 19.9, 19.6, 23.2,\n",
       "       29.8, 13.8, 13.3, 16.7, 12. , 14.6, 21.4, 23. , 23.7, 25. , 21.8,\n",
       "       20.6, 21.2, 19.1, 20.6, 15.2,  7. ,  8.1, 13.6, 20.1, 21.8, 24.5,\n",
       "       23.1, 19.7, 18.3, 21.2, 17.5, 16.8, 22.4, 20.6, 23.9, 22. , 11.9])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston['target']\n",
    "#타겟 데이터 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "536bcc64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD',\n",
       "       'TAX', 'PTRATIO', 'B', 'LSTAT'], dtype='<U7')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston['feature_names']\n",
    "#특성값(컬럼명) 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0e5aea",
   "metadata": {},
   "source": [
    "## 3. 데이터 전처리\n",
    "\n",
    "- 학습용 데이터라 전처리 필요 X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1d85b2",
   "metadata": {},
   "source": [
    "## 4. 탐색적 데이터 분석(EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "324fad65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>0.06263</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.593</td>\n",
       "      <td>69.1</td>\n",
       "      <td>2.4786</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>391.99</td>\n",
       "      <td>9.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>0.04527</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.120</td>\n",
       "      <td>76.7</td>\n",
       "      <td>2.2875</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>0.06076</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.976</td>\n",
       "      <td>91.0</td>\n",
       "      <td>2.1675</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>0.10959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.794</td>\n",
       "      <td>89.3</td>\n",
       "      <td>2.3889</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>393.45</td>\n",
       "      <td>6.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>0.04741</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.030</td>\n",
       "      <td>80.8</td>\n",
       "      <td>2.5050</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>7.88</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>506 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0    0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1    0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2    0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3    0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4    0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "..       ...   ...    ...   ...    ...    ...   ...     ...  ...    ...   \n",
       "501  0.06263   0.0  11.93   0.0  0.573  6.593  69.1  2.4786  1.0  273.0   \n",
       "502  0.04527   0.0  11.93   0.0  0.573  6.120  76.7  2.2875  1.0  273.0   \n",
       "503  0.06076   0.0  11.93   0.0  0.573  6.976  91.0  2.1675  1.0  273.0   \n",
       "504  0.10959   0.0  11.93   0.0  0.573  6.794  89.3  2.3889  1.0  273.0   \n",
       "505  0.04741   0.0  11.93   0.0  0.573  6.030  80.8  2.5050  1.0  273.0   \n",
       "\n",
       "     PTRATIO       B  LSTAT  \n",
       "0       15.3  396.90   4.98  \n",
       "1       17.8  396.90   9.14  \n",
       "2       17.8  392.83   4.03  \n",
       "3       18.7  394.63   2.94  \n",
       "4       18.7  396.90   5.33  \n",
       "..       ...     ...    ...  \n",
       "501     21.0  391.99   9.67  \n",
       "502     21.0  396.90   9.08  \n",
       "503     21.0  396.90   5.64  \n",
       "504     21.0  393.45   6.48  \n",
       "505     21.0  396.90   7.88  \n",
       "\n",
       "[506 rows x 13 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델을 학습시킬때 딕셔너리 형태로 학습시키는것이 불가능해 \n",
    "# 데이터프레임으로 변환해야함.\n",
    "\n",
    "x = pd.DataFrame(boston['data'],columns = boston['feature_names'])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05c71df4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>22.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>20.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>23.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>11.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>506 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     price\n",
       "0     24.0\n",
       "1     21.6\n",
       "2     34.7\n",
       "3     33.4\n",
       "4     36.2\n",
       "..     ...\n",
       "501   22.4\n",
       "502   20.6\n",
       "503   23.9\n",
       "504   22.0\n",
       "505   11.9\n",
       "\n",
       "[506 rows x 1 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = pd.DataFrame(boston['target'],columns = ['price'])\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16874dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 506 entries, 0 to 505\n",
      "Data columns (total 13 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   CRIM     506 non-null    float64\n",
      " 1   ZN       506 non-null    float64\n",
      " 2   INDUS    506 non-null    float64\n",
      " 3   CHAS     506 non-null    float64\n",
      " 4   NOX      506 non-null    float64\n",
      " 5   RM       506 non-null    float64\n",
      " 6   AGE      506 non-null    float64\n",
      " 7   DIS      506 non-null    float64\n",
      " 8   RAD      506 non-null    float64\n",
      " 9   TAX      506 non-null    float64\n",
      " 10  PTRATIO  506 non-null    float64\n",
      " 11  B        506 non-null    float64\n",
      " 12  LSTAT    506 non-null    float64\n",
      "dtypes: float64(13)\n",
      "memory usage: 51.5 KB\n"
     ]
    }
   ],
   "source": [
    "x.info()\n",
    "#정보 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1cb0a22d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 506 entries, 0 to 505\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   price   506 non-null    float64\n",
      "dtypes: float64(1)\n",
      "memory usage: 4.1 KB\n"
     ]
    }
   ],
   "source": [
    "y.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d4e93a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d95c7ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(x,y,\n",
    "                                                test_size=0.2,\n",
    "                                                random_state=0)\n",
    "#데이터를 train데이터와 test데이터로 분할,test데이터의 비율은 20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8dc203cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404, 13)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape\n",
    "#데이터 모형 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "429fb33b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404, 1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "346d9567",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102, 13)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba7a8733",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102, 1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62afece",
   "metadata": {},
   "source": [
    "### 5. 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "afab04d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c8f675b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "#선형회귀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "38f2b455",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(x_train,y_train)\n",
    "#모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6f29d546",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.19443447e-01,  4.47799511e-02,  5.48526168e-03,\n",
       "         2.34080361e+00, -1.61236043e+01,  3.70870901e+00,\n",
       "        -3.12108178e-03, -1.38639737e+00,  2.44178327e-01,\n",
       "        -1.09896366e-02, -1.04592119e+00,  8.11010693e-03,\n",
       "        -4.92792725e-01]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.coef_\n",
    "#가중치 확인 함수\n",
    "#결과는 feature(특성,칼럼명)의 개수만큼 나옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b09edb7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([38.09169493])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.intercept_\n",
    "#편향값 확인 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8d26d1",
   "metadata": {},
   "source": [
    "### 6. 모델 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f0f10946",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = lr.predict(x_train)\n",
    "test = lr.predict(x_test)\n",
    "#예측값 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "23bbe05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_score = mean_squared_error(train,y_train)\n",
    "test_score = mean_squared_error(test,y_test)\n",
    "\n",
    "#기준은 MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "deaeef7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.326470203585725\n"
     ]
    }
   ],
   "source": [
    "print(train_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "27d70e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33.44897999767653\n"
     ]
    }
   ],
   "source": [
    "print(test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "532f0373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse: 4.396188144698282\n"
     ]
    }
   ],
   "source": [
    "print('rmse:',train_score ** 0.5)\n",
    "#RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "989af8fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse: 5.783509315085135\n"
     ]
    }
   ],
   "source": [
    "print('rmse:',test_score ** 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1349d654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.387321170386853\n"
     ]
    }
   ],
   "source": [
    "print(train_score ** 0.5 - test_score ** 0.5)\n",
    "#오차"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2300d016",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7730135569264234"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(x_train,y_train)\n",
    "#r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aef954c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5892223849182507"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0a013ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f48a65cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_score = mean_absolute_error(train,y_train)\n",
    "test_score = mean_absolute_error(test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "351bda99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.10315639657943\n"
     ]
    }
   ],
   "source": [
    "print(train_score)\n",
    "#MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "19c99e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.842909220444498\n"
     ]
    }
   ],
   "source": [
    "print(test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "95c5a6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_percentage_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b0bf4ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_score = mean_absolute_percentage_error(train,y_train)\n",
    "test_score = mean_absolute_percentage_error(test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "71a5b942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18344726250983412\n"
     ]
    }
   ],
   "source": [
    "print(train_score)\n",
    "#MAPE(평균절대비율오차)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "86894482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.40440562284366\n"
     ]
    }
   ],
   "source": [
    "print(test_score)\n",
    "#MAPE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e7dac6",
   "metadata": {},
   "source": [
    "### 선형회귀 모델 특성확장\n",
    "- 선형회귀는 특성이 적으면 다른 알고리즘에 비해 성능이 낮게 나오기 때문에 특성확장을 사용해서 모델의 복잡도를 증가\n",
    "- 각 컬럼들의 데이터를 곱하여 새로운 특성으로 확장시켜보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "739bebfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_x_train = x_train.copy()\n",
    "#copy() = 복제. \n",
    "#데이터의 값을 바꾸는 상황에서 데이터의 원형을 남기고 싶을때 데이터를 복제한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d93ffac4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>0.35809</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.507</td>\n",
       "      <td>6.951</td>\n",
       "      <td>88.5</td>\n",
       "      <td>2.8617</td>\n",
       "      <td>8.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>17.4</td>\n",
       "      <td>391.70</td>\n",
       "      <td>9.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.15876</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.81</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.413</td>\n",
       "      <td>5.961</td>\n",
       "      <td>17.5</td>\n",
       "      <td>5.2873</td>\n",
       "      <td>4.0</td>\n",
       "      <td>305.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>376.94</td>\n",
       "      <td>9.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>0.11329</td>\n",
       "      <td>30.0</td>\n",
       "      <td>4.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.428</td>\n",
       "      <td>6.897</td>\n",
       "      <td>54.3</td>\n",
       "      <td>6.3361</td>\n",
       "      <td>6.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>16.6</td>\n",
       "      <td>391.25</td>\n",
       "      <td>11.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.08829</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>6.012</td>\n",
       "      <td>66.6</td>\n",
       "      <td>5.5605</td>\n",
       "      <td>5.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>395.60</td>\n",
       "      <td>12.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>25.94060</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.679</td>\n",
       "      <td>5.304</td>\n",
       "      <td>89.1</td>\n",
       "      <td>1.6475</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>127.36</td>\n",
       "      <td>26.64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS   RAD    TAX  \\\n",
       "220   0.35809   0.0   6.20   1.0  0.507  6.951  88.5  2.8617   8.0  307.0   \n",
       "71    0.15876   0.0  10.81   0.0  0.413  5.961  17.5  5.2873   4.0  305.0   \n",
       "240   0.11329  30.0   4.93   0.0  0.428  6.897  54.3  6.3361   6.0  300.0   \n",
       "6     0.08829  12.5   7.87   0.0  0.524  6.012  66.6  5.5605   5.0  311.0   \n",
       "417  25.94060   0.0  18.10   0.0  0.679  5.304  89.1  1.6475  24.0  666.0   \n",
       "\n",
       "     PTRATIO       B  LSTAT  \n",
       "220     17.4  391.70   9.71  \n",
       "71      19.2  376.94   9.88  \n",
       "240     16.6  391.25  11.38  \n",
       "6       15.2  395.60  12.43  \n",
       "417     20.2  127.36  26.64  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extended_x_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fcd03cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\271221717.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1+'x'+col2] = x_train[col1]*x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\271221717.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1+'x'+col2] = x_train[col1]*x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\271221717.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1+'x'+col2] = x_train[col1]*x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\271221717.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1+'x'+col2] = x_train[col1]*x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\271221717.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1+'x'+col2] = x_train[col1]*x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\271221717.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1+'x'+col2] = x_train[col1]*x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\271221717.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1+'x'+col2] = x_train[col1]*x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\271221717.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1+'x'+col2] = x_train[col1]*x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\271221717.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1+'x'+col2] = x_train[col1]*x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\271221717.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1+'x'+col2] = x_train[col1]*x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\271221717.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1+'x'+col2] = x_train[col1]*x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\271221717.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1+'x'+col2] = x_train[col1]*x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\271221717.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1+'x'+col2] = x_train[col1]*x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\271221717.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1+'x'+col2] = x_train[col1]*x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\271221717.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1+'x'+col2] = x_train[col1]*x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\271221717.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1+'x'+col2] = x_train[col1]*x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\271221717.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1+'x'+col2] = x_train[col1]*x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\271221717.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1+'x'+col2] = x_train[col1]*x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\271221717.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1+'x'+col2] = x_train[col1]*x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\271221717.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1+'x'+col2] = x_train[col1]*x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\271221717.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1+'x'+col2] = x_train[col1]*x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\271221717.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1+'x'+col2] = x_train[col1]*x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\271221717.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1+'x'+col2] = x_train[col1]*x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\271221717.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1+'x'+col2] = x_train[col1]*x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\271221717.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1+'x'+col2] = x_train[col1]*x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\271221717.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1+'x'+col2] = x_train[col1]*x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\271221717.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1+'x'+col2] = x_train[col1]*x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\271221717.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1+'x'+col2] = x_train[col1]*x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\271221717.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1+'x'+col2] = x_train[col1]*x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\271221717.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1+'x'+col2] = x_train[col1]*x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\271221717.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1+'x'+col2] = x_train[col1]*x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\271221717.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1+'x'+col2] = x_train[col1]*x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\271221717.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1+'x'+col2] = x_train[col1]*x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\271221717.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1+'x'+col2] = x_train[col1]*x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\271221717.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1+'x'+col2] = x_train[col1]*x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\271221717.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1+'x'+col2] = x_train[col1]*x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\271221717.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1+'x'+col2] = x_train[col1]*x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\271221717.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1+'x'+col2] = x_train[col1]*x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\271221717.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1+'x'+col2] = x_train[col1]*x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\271221717.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1+'x'+col2] = x_train[col1]*x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\271221717.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1+'x'+col2] = x_train[col1]*x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\271221717.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1+'x'+col2] = x_train[col1]*x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\271221717.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1+'x'+col2] = x_train[col1]*x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\271221717.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1+'x'+col2] = x_train[col1]*x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\271221717.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1+'x'+col2] = x_train[col1]*x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\271221717.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1+'x'+col2] = x_train[col1]*x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\271221717.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1+'x'+col2] = x_train[col1]*x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\271221717.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1+'x'+col2] = x_train[col1]*x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\271221717.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1+'x'+col2] = x_train[col1]*x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\271221717.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1+'x'+col2] = x_train[col1]*x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\271221717.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1+'x'+col2] = x_train[col1]*x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\271221717.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1+'x'+col2] = x_train[col1]*x_train[col2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\271221717.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1+'x'+col2] = x_train[col1]*x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\271221717.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1+'x'+col2] = x_train[col1]*x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\271221717.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1+'x'+col2] = x_train[col1]*x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\271221717.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1+'x'+col2] = x_train[col1]*x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\271221717.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1+'x'+col2] = x_train[col1]*x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\271221717.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1+'x'+col2] = x_train[col1]*x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\271221717.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1+'x'+col2] = x_train[col1]*x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\271221717.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1+'x'+col2] = x_train[col1]*x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\271221717.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1+'x'+col2] = x_train[col1]*x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\271221717.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1+'x'+col2] = x_train[col1]*x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\271221717.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1+'x'+col2] = x_train[col1]*x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\271221717.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1+'x'+col2] = x_train[col1]*x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\271221717.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1+'x'+col2] = x_train[col1]*x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\271221717.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1+'x'+col2] = x_train[col1]*x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\271221717.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1+'x'+col2] = x_train[col1]*x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\271221717.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1+'x'+col2] = x_train[col1]*x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\271221717.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1+'x'+col2] = x_train[col1]*x_train[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\271221717.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_train[col1+'x'+col2] = x_train[col1]*x_train[col2]\n"
     ]
    }
   ],
   "source": [
    "#extended_x_train의 각 컬럼들을 서로 한번씩 곱해서 새로운 컬럼 추가.\n",
    "#train 데이터셋 특성 확장 코드\n",
    "\n",
    "for col1 in x_train.columns: #13번 반복 (x_train의 칼럼수)\n",
    "    for col2 in x_train.columns: #13번 반복\n",
    "        extended_x_train[col1+'x'+col2] = x_train[col1]*x_train[col2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "382a9bec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>...</th>\n",
       "      <th>LSTATxCHAS</th>\n",
       "      <th>LSTATxNOX</th>\n",
       "      <th>LSTATxRM</th>\n",
       "      <th>LSTATxAGE</th>\n",
       "      <th>LSTATxDIS</th>\n",
       "      <th>LSTATxRAD</th>\n",
       "      <th>LSTATxTAX</th>\n",
       "      <th>LSTATxPTRATIO</th>\n",
       "      <th>LSTATxB</th>\n",
       "      <th>LSTATxLSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>0.35809</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.507</td>\n",
       "      <td>6.951</td>\n",
       "      <td>88.5</td>\n",
       "      <td>2.8617</td>\n",
       "      <td>8.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9.71</td>\n",
       "      <td>4.92297</td>\n",
       "      <td>67.49421</td>\n",
       "      <td>859.335</td>\n",
       "      <td>27.787107</td>\n",
       "      <td>77.68</td>\n",
       "      <td>2980.97</td>\n",
       "      <td>168.954</td>\n",
       "      <td>3803.4070</td>\n",
       "      <td>94.2841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.15876</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.81</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.413</td>\n",
       "      <td>5.961</td>\n",
       "      <td>17.5</td>\n",
       "      <td>5.2873</td>\n",
       "      <td>4.0</td>\n",
       "      <td>305.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.08044</td>\n",
       "      <td>58.89468</td>\n",
       "      <td>172.900</td>\n",
       "      <td>52.238524</td>\n",
       "      <td>39.52</td>\n",
       "      <td>3013.40</td>\n",
       "      <td>189.696</td>\n",
       "      <td>3724.1672</td>\n",
       "      <td>97.6144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>0.11329</td>\n",
       "      <td>30.0</td>\n",
       "      <td>4.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.428</td>\n",
       "      <td>6.897</td>\n",
       "      <td>54.3</td>\n",
       "      <td>6.3361</td>\n",
       "      <td>6.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.87064</td>\n",
       "      <td>78.48786</td>\n",
       "      <td>617.934</td>\n",
       "      <td>72.104818</td>\n",
       "      <td>68.28</td>\n",
       "      <td>3414.00</td>\n",
       "      <td>188.908</td>\n",
       "      <td>4452.4250</td>\n",
       "      <td>129.5044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.08829</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>6.012</td>\n",
       "      <td>66.6</td>\n",
       "      <td>5.5605</td>\n",
       "      <td>5.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.51332</td>\n",
       "      <td>74.72916</td>\n",
       "      <td>827.838</td>\n",
       "      <td>69.117015</td>\n",
       "      <td>62.15</td>\n",
       "      <td>3865.73</td>\n",
       "      <td>188.936</td>\n",
       "      <td>4917.3080</td>\n",
       "      <td>154.5049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>25.94060</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.679</td>\n",
       "      <td>5.304</td>\n",
       "      <td>89.1</td>\n",
       "      <td>1.6475</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>18.08856</td>\n",
       "      <td>141.29856</td>\n",
       "      <td>2373.624</td>\n",
       "      <td>43.889400</td>\n",
       "      <td>639.36</td>\n",
       "      <td>17742.24</td>\n",
       "      <td>538.128</td>\n",
       "      <td>3392.8704</td>\n",
       "      <td>709.6896</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 182 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS   RAD    TAX  \\\n",
       "220   0.35809   0.0   6.20   1.0  0.507  6.951  88.5  2.8617   8.0  307.0   \n",
       "71    0.15876   0.0  10.81   0.0  0.413  5.961  17.5  5.2873   4.0  305.0   \n",
       "240   0.11329  30.0   4.93   0.0  0.428  6.897  54.3  6.3361   6.0  300.0   \n",
       "6     0.08829  12.5   7.87   0.0  0.524  6.012  66.6  5.5605   5.0  311.0   \n",
       "417  25.94060   0.0  18.10   0.0  0.679  5.304  89.1  1.6475  24.0  666.0   \n",
       "\n",
       "     ...  LSTATxCHAS  LSTATxNOX   LSTATxRM  LSTATxAGE  LSTATxDIS  LSTATxRAD  \\\n",
       "220  ...        9.71    4.92297   67.49421    859.335  27.787107      77.68   \n",
       "71   ...        0.00    4.08044   58.89468    172.900  52.238524      39.52   \n",
       "240  ...        0.00    4.87064   78.48786    617.934  72.104818      68.28   \n",
       "6    ...        0.00    6.51332   74.72916    827.838  69.117015      62.15   \n",
       "417  ...        0.00   18.08856  141.29856   2373.624  43.889400     639.36   \n",
       "\n",
       "     LSTATxTAX  LSTATxPTRATIO    LSTATxB  LSTATxLSTAT  \n",
       "220    2980.97        168.954  3803.4070      94.2841  \n",
       "71     3013.40        189.696  3724.1672      97.6144  \n",
       "240    3414.00        188.908  4452.4250     129.5044  \n",
       "6      3865.73        188.936  4917.3080     154.5049  \n",
       "417   17742.24        538.128  3392.8704     709.6896  \n",
       "\n",
       "[5 rows x 182 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extended_x_train.head()\n",
    "#columns의 수는 (원래 칼럼수 * 월래 칼럼수) + 원래 칼럼수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e86359b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns',None)\n",
    "#데이터 확인시 ... 없애고 모든 컬럼을 확인하는 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "262fc614",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>CRIMxCRIM</th>\n",
       "      <th>CRIMxZN</th>\n",
       "      <th>CRIMxINDUS</th>\n",
       "      <th>CRIMxCHAS</th>\n",
       "      <th>CRIMxNOX</th>\n",
       "      <th>CRIMxRM</th>\n",
       "      <th>CRIMxAGE</th>\n",
       "      <th>CRIMxDIS</th>\n",
       "      <th>CRIMxRAD</th>\n",
       "      <th>CRIMxTAX</th>\n",
       "      <th>CRIMxPTRATIO</th>\n",
       "      <th>CRIMxB</th>\n",
       "      <th>CRIMxLSTAT</th>\n",
       "      <th>ZNxCRIM</th>\n",
       "      <th>ZNxZN</th>\n",
       "      <th>ZNxINDUS</th>\n",
       "      <th>ZNxCHAS</th>\n",
       "      <th>ZNxNOX</th>\n",
       "      <th>ZNxRM</th>\n",
       "      <th>ZNxAGE</th>\n",
       "      <th>ZNxDIS</th>\n",
       "      <th>ZNxRAD</th>\n",
       "      <th>ZNxTAX</th>\n",
       "      <th>ZNxPTRATIO</th>\n",
       "      <th>ZNxB</th>\n",
       "      <th>ZNxLSTAT</th>\n",
       "      <th>INDUSxCRIM</th>\n",
       "      <th>INDUSxZN</th>\n",
       "      <th>INDUSxINDUS</th>\n",
       "      <th>INDUSxCHAS</th>\n",
       "      <th>INDUSxNOX</th>\n",
       "      <th>INDUSxRM</th>\n",
       "      <th>INDUSxAGE</th>\n",
       "      <th>INDUSxDIS</th>\n",
       "      <th>INDUSxRAD</th>\n",
       "      <th>INDUSxTAX</th>\n",
       "      <th>INDUSxPTRATIO</th>\n",
       "      <th>INDUSxB</th>\n",
       "      <th>INDUSxLSTAT</th>\n",
       "      <th>CHASxCRIM</th>\n",
       "      <th>CHASxZN</th>\n",
       "      <th>CHASxINDUS</th>\n",
       "      <th>CHASxCHAS</th>\n",
       "      <th>CHASxNOX</th>\n",
       "      <th>CHASxRM</th>\n",
       "      <th>CHASxAGE</th>\n",
       "      <th>CHASxDIS</th>\n",
       "      <th>CHASxRAD</th>\n",
       "      <th>CHASxTAX</th>\n",
       "      <th>CHASxPTRATIO</th>\n",
       "      <th>CHASxB</th>\n",
       "      <th>CHASxLSTAT</th>\n",
       "      <th>NOXxCRIM</th>\n",
       "      <th>NOXxZN</th>\n",
       "      <th>NOXxINDUS</th>\n",
       "      <th>NOXxCHAS</th>\n",
       "      <th>NOXxNOX</th>\n",
       "      <th>NOXxRM</th>\n",
       "      <th>NOXxAGE</th>\n",
       "      <th>NOXxDIS</th>\n",
       "      <th>NOXxRAD</th>\n",
       "      <th>NOXxTAX</th>\n",
       "      <th>NOXxPTRATIO</th>\n",
       "      <th>NOXxB</th>\n",
       "      <th>NOXxLSTAT</th>\n",
       "      <th>RMxCRIM</th>\n",
       "      <th>RMxZN</th>\n",
       "      <th>RMxINDUS</th>\n",
       "      <th>RMxCHAS</th>\n",
       "      <th>RMxNOX</th>\n",
       "      <th>RMxRM</th>\n",
       "      <th>RMxAGE</th>\n",
       "      <th>RMxDIS</th>\n",
       "      <th>RMxRAD</th>\n",
       "      <th>RMxTAX</th>\n",
       "      <th>RMxPTRATIO</th>\n",
       "      <th>RMxB</th>\n",
       "      <th>RMxLSTAT</th>\n",
       "      <th>AGExCRIM</th>\n",
       "      <th>AGExZN</th>\n",
       "      <th>AGExINDUS</th>\n",
       "      <th>AGExCHAS</th>\n",
       "      <th>AGExNOX</th>\n",
       "      <th>AGExRM</th>\n",
       "      <th>AGExAGE</th>\n",
       "      <th>AGExDIS</th>\n",
       "      <th>AGExRAD</th>\n",
       "      <th>AGExTAX</th>\n",
       "      <th>AGExPTRATIO</th>\n",
       "      <th>AGExB</th>\n",
       "      <th>AGExLSTAT</th>\n",
       "      <th>DISxCRIM</th>\n",
       "      <th>DISxZN</th>\n",
       "      <th>DISxINDUS</th>\n",
       "      <th>DISxCHAS</th>\n",
       "      <th>DISxNOX</th>\n",
       "      <th>DISxRM</th>\n",
       "      <th>DISxAGE</th>\n",
       "      <th>DISxDIS</th>\n",
       "      <th>DISxRAD</th>\n",
       "      <th>DISxTAX</th>\n",
       "      <th>DISxPTRATIO</th>\n",
       "      <th>DISxB</th>\n",
       "      <th>DISxLSTAT</th>\n",
       "      <th>RADxCRIM</th>\n",
       "      <th>RADxZN</th>\n",
       "      <th>RADxINDUS</th>\n",
       "      <th>RADxCHAS</th>\n",
       "      <th>RADxNOX</th>\n",
       "      <th>RADxRM</th>\n",
       "      <th>RADxAGE</th>\n",
       "      <th>RADxDIS</th>\n",
       "      <th>RADxRAD</th>\n",
       "      <th>RADxTAX</th>\n",
       "      <th>RADxPTRATIO</th>\n",
       "      <th>RADxB</th>\n",
       "      <th>RADxLSTAT</th>\n",
       "      <th>TAXxCRIM</th>\n",
       "      <th>TAXxZN</th>\n",
       "      <th>TAXxINDUS</th>\n",
       "      <th>TAXxCHAS</th>\n",
       "      <th>TAXxNOX</th>\n",
       "      <th>TAXxRM</th>\n",
       "      <th>TAXxAGE</th>\n",
       "      <th>TAXxDIS</th>\n",
       "      <th>TAXxRAD</th>\n",
       "      <th>TAXxTAX</th>\n",
       "      <th>TAXxPTRATIO</th>\n",
       "      <th>TAXxB</th>\n",
       "      <th>TAXxLSTAT</th>\n",
       "      <th>PTRATIOxCRIM</th>\n",
       "      <th>PTRATIOxZN</th>\n",
       "      <th>PTRATIOxINDUS</th>\n",
       "      <th>PTRATIOxCHAS</th>\n",
       "      <th>PTRATIOxNOX</th>\n",
       "      <th>PTRATIOxRM</th>\n",
       "      <th>PTRATIOxAGE</th>\n",
       "      <th>PTRATIOxDIS</th>\n",
       "      <th>PTRATIOxRAD</th>\n",
       "      <th>PTRATIOxTAX</th>\n",
       "      <th>PTRATIOxPTRATIO</th>\n",
       "      <th>PTRATIOxB</th>\n",
       "      <th>PTRATIOxLSTAT</th>\n",
       "      <th>BxCRIM</th>\n",
       "      <th>BxZN</th>\n",
       "      <th>BxINDUS</th>\n",
       "      <th>BxCHAS</th>\n",
       "      <th>BxNOX</th>\n",
       "      <th>BxRM</th>\n",
       "      <th>BxAGE</th>\n",
       "      <th>BxDIS</th>\n",
       "      <th>BxRAD</th>\n",
       "      <th>BxTAX</th>\n",
       "      <th>BxPTRATIO</th>\n",
       "      <th>BxB</th>\n",
       "      <th>BxLSTAT</th>\n",
       "      <th>LSTATxCRIM</th>\n",
       "      <th>LSTATxZN</th>\n",
       "      <th>LSTATxINDUS</th>\n",
       "      <th>LSTATxCHAS</th>\n",
       "      <th>LSTATxNOX</th>\n",
       "      <th>LSTATxRM</th>\n",
       "      <th>LSTATxAGE</th>\n",
       "      <th>LSTATxDIS</th>\n",
       "      <th>LSTATxRAD</th>\n",
       "      <th>LSTATxTAX</th>\n",
       "      <th>LSTATxPTRATIO</th>\n",
       "      <th>LSTATxB</th>\n",
       "      <th>LSTATxLSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>0.35809</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.507</td>\n",
       "      <td>6.951</td>\n",
       "      <td>88.5</td>\n",
       "      <td>2.8617</td>\n",
       "      <td>8.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>17.4</td>\n",
       "      <td>391.70</td>\n",
       "      <td>9.71</td>\n",
       "      <td>0.128228</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.220158</td>\n",
       "      <td>0.35809</td>\n",
       "      <td>0.181552</td>\n",
       "      <td>2.489084</td>\n",
       "      <td>31.690965</td>\n",
       "      <td>1.024746</td>\n",
       "      <td>2.86472</td>\n",
       "      <td>109.93363</td>\n",
       "      <td>6.230766</td>\n",
       "      <td>140.263853</td>\n",
       "      <td>3.477054</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.220158</td>\n",
       "      <td>0.000</td>\n",
       "      <td>38.4400</td>\n",
       "      <td>6.2</td>\n",
       "      <td>3.14340</td>\n",
       "      <td>43.09620</td>\n",
       "      <td>548.700</td>\n",
       "      <td>17.742540</td>\n",
       "      <td>49.60</td>\n",
       "      <td>1903.40</td>\n",
       "      <td>107.880</td>\n",
       "      <td>2428.5400</td>\n",
       "      <td>60.2020</td>\n",
       "      <td>0.35809</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.507</td>\n",
       "      <td>6.951</td>\n",
       "      <td>88.5</td>\n",
       "      <td>2.8617</td>\n",
       "      <td>8.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>17.4</td>\n",
       "      <td>391.7</td>\n",
       "      <td>9.71</td>\n",
       "      <td>0.181552</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.14340</td>\n",
       "      <td>0.507</td>\n",
       "      <td>0.257049</td>\n",
       "      <td>3.524157</td>\n",
       "      <td>44.8695</td>\n",
       "      <td>1.450882</td>\n",
       "      <td>4.056</td>\n",
       "      <td>155.649</td>\n",
       "      <td>8.8218</td>\n",
       "      <td>198.59190</td>\n",
       "      <td>4.92297</td>\n",
       "      <td>2.489084</td>\n",
       "      <td>0.00</td>\n",
       "      <td>43.09620</td>\n",
       "      <td>6.951</td>\n",
       "      <td>3.524157</td>\n",
       "      <td>48.316401</td>\n",
       "      <td>615.1635</td>\n",
       "      <td>19.891677</td>\n",
       "      <td>55.608</td>\n",
       "      <td>2133.957</td>\n",
       "      <td>120.9474</td>\n",
       "      <td>2722.70670</td>\n",
       "      <td>67.49421</td>\n",
       "      <td>31.690965</td>\n",
       "      <td>0.0</td>\n",
       "      <td>548.700</td>\n",
       "      <td>88.5</td>\n",
       "      <td>44.8695</td>\n",
       "      <td>615.1635</td>\n",
       "      <td>7832.25</td>\n",
       "      <td>253.26045</td>\n",
       "      <td>708.0</td>\n",
       "      <td>27169.5</td>\n",
       "      <td>1539.90</td>\n",
       "      <td>34665.450</td>\n",
       "      <td>859.335</td>\n",
       "      <td>1.024746</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>17.742540</td>\n",
       "      <td>2.8617</td>\n",
       "      <td>1.450882</td>\n",
       "      <td>19.891677</td>\n",
       "      <td>253.26045</td>\n",
       "      <td>8.189327</td>\n",
       "      <td>22.8936</td>\n",
       "      <td>878.5419</td>\n",
       "      <td>49.79358</td>\n",
       "      <td>1120.927890</td>\n",
       "      <td>27.787107</td>\n",
       "      <td>2.86472</td>\n",
       "      <td>0.0</td>\n",
       "      <td>49.60</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.056</td>\n",
       "      <td>55.608</td>\n",
       "      <td>708.0</td>\n",
       "      <td>22.8936</td>\n",
       "      <td>64.0</td>\n",
       "      <td>2456.0</td>\n",
       "      <td>139.2</td>\n",
       "      <td>3133.60</td>\n",
       "      <td>77.68</td>\n",
       "      <td>109.93363</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1903.40</td>\n",
       "      <td>307.0</td>\n",
       "      <td>155.649</td>\n",
       "      <td>2133.957</td>\n",
       "      <td>27169.5</td>\n",
       "      <td>878.5419</td>\n",
       "      <td>2456.0</td>\n",
       "      <td>94249.0</td>\n",
       "      <td>5341.8</td>\n",
       "      <td>120251.90</td>\n",
       "      <td>2980.97</td>\n",
       "      <td>6.230766</td>\n",
       "      <td>0.0</td>\n",
       "      <td>107.880</td>\n",
       "      <td>17.4</td>\n",
       "      <td>8.8218</td>\n",
       "      <td>120.9474</td>\n",
       "      <td>1539.90</td>\n",
       "      <td>49.79358</td>\n",
       "      <td>139.2</td>\n",
       "      <td>5341.8</td>\n",
       "      <td>302.76</td>\n",
       "      <td>6815.580</td>\n",
       "      <td>168.954</td>\n",
       "      <td>140.263853</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2428.5400</td>\n",
       "      <td>391.7</td>\n",
       "      <td>198.59190</td>\n",
       "      <td>2722.70670</td>\n",
       "      <td>34665.450</td>\n",
       "      <td>1120.927890</td>\n",
       "      <td>3133.60</td>\n",
       "      <td>120251.90</td>\n",
       "      <td>6815.580</td>\n",
       "      <td>153428.8900</td>\n",
       "      <td>3803.4070</td>\n",
       "      <td>3.477054</td>\n",
       "      <td>0.000</td>\n",
       "      <td>60.2020</td>\n",
       "      <td>9.71</td>\n",
       "      <td>4.92297</td>\n",
       "      <td>67.49421</td>\n",
       "      <td>859.335</td>\n",
       "      <td>27.787107</td>\n",
       "      <td>77.68</td>\n",
       "      <td>2980.97</td>\n",
       "      <td>168.954</td>\n",
       "      <td>3803.4070</td>\n",
       "      <td>94.2841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.15876</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.81</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.413</td>\n",
       "      <td>5.961</td>\n",
       "      <td>17.5</td>\n",
       "      <td>5.2873</td>\n",
       "      <td>4.0</td>\n",
       "      <td>305.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>376.94</td>\n",
       "      <td>9.88</td>\n",
       "      <td>0.025205</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.716196</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.065568</td>\n",
       "      <td>0.946368</td>\n",
       "      <td>2.778300</td>\n",
       "      <td>0.839412</td>\n",
       "      <td>0.63504</td>\n",
       "      <td>48.42180</td>\n",
       "      <td>3.048192</td>\n",
       "      <td>59.842994</td>\n",
       "      <td>1.568549</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.716196</td>\n",
       "      <td>0.000</td>\n",
       "      <td>116.8561</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.46453</td>\n",
       "      <td>64.43841</td>\n",
       "      <td>189.175</td>\n",
       "      <td>57.155713</td>\n",
       "      <td>43.24</td>\n",
       "      <td>3297.05</td>\n",
       "      <td>207.552</td>\n",
       "      <td>4074.7214</td>\n",
       "      <td>106.8028</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.065568</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.46453</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.170569</td>\n",
       "      <td>2.461893</td>\n",
       "      <td>7.2275</td>\n",
       "      <td>2.183655</td>\n",
       "      <td>1.652</td>\n",
       "      <td>125.965</td>\n",
       "      <td>7.9296</td>\n",
       "      <td>155.67622</td>\n",
       "      <td>4.08044</td>\n",
       "      <td>0.946368</td>\n",
       "      <td>0.00</td>\n",
       "      <td>64.43841</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.461893</td>\n",
       "      <td>35.533521</td>\n",
       "      <td>104.3175</td>\n",
       "      <td>31.517595</td>\n",
       "      <td>23.844</td>\n",
       "      <td>1818.105</td>\n",
       "      <td>114.4512</td>\n",
       "      <td>2246.93934</td>\n",
       "      <td>58.89468</td>\n",
       "      <td>2.778300</td>\n",
       "      <td>0.0</td>\n",
       "      <td>189.175</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.2275</td>\n",
       "      <td>104.3175</td>\n",
       "      <td>306.25</td>\n",
       "      <td>92.52775</td>\n",
       "      <td>70.0</td>\n",
       "      <td>5337.5</td>\n",
       "      <td>336.00</td>\n",
       "      <td>6596.450</td>\n",
       "      <td>172.900</td>\n",
       "      <td>0.839412</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>57.155713</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2.183655</td>\n",
       "      <td>31.517595</td>\n",
       "      <td>92.52775</td>\n",
       "      <td>27.955541</td>\n",
       "      <td>21.1492</td>\n",
       "      <td>1612.6265</td>\n",
       "      <td>101.51616</td>\n",
       "      <td>1992.994862</td>\n",
       "      <td>52.238524</td>\n",
       "      <td>0.63504</td>\n",
       "      <td>0.0</td>\n",
       "      <td>43.24</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.652</td>\n",
       "      <td>23.844</td>\n",
       "      <td>70.0</td>\n",
       "      <td>21.1492</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1220.0</td>\n",
       "      <td>76.8</td>\n",
       "      <td>1507.76</td>\n",
       "      <td>39.52</td>\n",
       "      <td>48.42180</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3297.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>125.965</td>\n",
       "      <td>1818.105</td>\n",
       "      <td>5337.5</td>\n",
       "      <td>1612.6265</td>\n",
       "      <td>1220.0</td>\n",
       "      <td>93025.0</td>\n",
       "      <td>5856.0</td>\n",
       "      <td>114966.70</td>\n",
       "      <td>3013.40</td>\n",
       "      <td>3.048192</td>\n",
       "      <td>0.0</td>\n",
       "      <td>207.552</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.9296</td>\n",
       "      <td>114.4512</td>\n",
       "      <td>336.00</td>\n",
       "      <td>101.51616</td>\n",
       "      <td>76.8</td>\n",
       "      <td>5856.0</td>\n",
       "      <td>368.64</td>\n",
       "      <td>7237.248</td>\n",
       "      <td>189.696</td>\n",
       "      <td>59.842994</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4074.7214</td>\n",
       "      <td>0.0</td>\n",
       "      <td>155.67622</td>\n",
       "      <td>2246.93934</td>\n",
       "      <td>6596.450</td>\n",
       "      <td>1992.994862</td>\n",
       "      <td>1507.76</td>\n",
       "      <td>114966.70</td>\n",
       "      <td>7237.248</td>\n",
       "      <td>142083.7636</td>\n",
       "      <td>3724.1672</td>\n",
       "      <td>1.568549</td>\n",
       "      <td>0.000</td>\n",
       "      <td>106.8028</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.08044</td>\n",
       "      <td>58.89468</td>\n",
       "      <td>172.900</td>\n",
       "      <td>52.238524</td>\n",
       "      <td>39.52</td>\n",
       "      <td>3013.40</td>\n",
       "      <td>189.696</td>\n",
       "      <td>3724.1672</td>\n",
       "      <td>97.6144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>0.11329</td>\n",
       "      <td>30.0</td>\n",
       "      <td>4.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.428</td>\n",
       "      <td>6.897</td>\n",
       "      <td>54.3</td>\n",
       "      <td>6.3361</td>\n",
       "      <td>6.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>16.6</td>\n",
       "      <td>391.25</td>\n",
       "      <td>11.38</td>\n",
       "      <td>0.012835</td>\n",
       "      <td>3.398700</td>\n",
       "      <td>0.558520</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.048488</td>\n",
       "      <td>0.781361</td>\n",
       "      <td>6.151647</td>\n",
       "      <td>0.717817</td>\n",
       "      <td>0.67974</td>\n",
       "      <td>33.98700</td>\n",
       "      <td>1.880614</td>\n",
       "      <td>44.324713</td>\n",
       "      <td>1.289240</td>\n",
       "      <td>3.398700</td>\n",
       "      <td>900.00</td>\n",
       "      <td>147.900</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.84</td>\n",
       "      <td>206.91</td>\n",
       "      <td>1629.0</td>\n",
       "      <td>190.08300</td>\n",
       "      <td>180.0</td>\n",
       "      <td>9000.0</td>\n",
       "      <td>498.0</td>\n",
       "      <td>11737.5</td>\n",
       "      <td>341.400</td>\n",
       "      <td>0.558520</td>\n",
       "      <td>147.900</td>\n",
       "      <td>24.3049</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.11004</td>\n",
       "      <td>34.00221</td>\n",
       "      <td>267.699</td>\n",
       "      <td>31.236973</td>\n",
       "      <td>29.58</td>\n",
       "      <td>1479.00</td>\n",
       "      <td>81.838</td>\n",
       "      <td>1928.8625</td>\n",
       "      <td>56.1034</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.048488</td>\n",
       "      <td>12.84</td>\n",
       "      <td>2.11004</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.183184</td>\n",
       "      <td>2.951916</td>\n",
       "      <td>23.2404</td>\n",
       "      <td>2.711851</td>\n",
       "      <td>2.568</td>\n",
       "      <td>128.400</td>\n",
       "      <td>7.1048</td>\n",
       "      <td>167.45500</td>\n",
       "      <td>4.87064</td>\n",
       "      <td>0.781361</td>\n",
       "      <td>206.91</td>\n",
       "      <td>34.00221</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.951916</td>\n",
       "      <td>47.568609</td>\n",
       "      <td>374.5071</td>\n",
       "      <td>43.700082</td>\n",
       "      <td>41.382</td>\n",
       "      <td>2069.100</td>\n",
       "      <td>114.4902</td>\n",
       "      <td>2698.45125</td>\n",
       "      <td>78.48786</td>\n",
       "      <td>6.151647</td>\n",
       "      <td>1629.0</td>\n",
       "      <td>267.699</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.2404</td>\n",
       "      <td>374.5071</td>\n",
       "      <td>2948.49</td>\n",
       "      <td>344.05023</td>\n",
       "      <td>325.8</td>\n",
       "      <td>16290.0</td>\n",
       "      <td>901.38</td>\n",
       "      <td>21244.875</td>\n",
       "      <td>617.934</td>\n",
       "      <td>0.717817</td>\n",
       "      <td>190.08300</td>\n",
       "      <td>31.236973</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2.711851</td>\n",
       "      <td>43.700082</td>\n",
       "      <td>344.05023</td>\n",
       "      <td>40.146163</td>\n",
       "      <td>38.0166</td>\n",
       "      <td>1900.8300</td>\n",
       "      <td>105.17926</td>\n",
       "      <td>2478.999125</td>\n",
       "      <td>72.104818</td>\n",
       "      <td>0.67974</td>\n",
       "      <td>180.0</td>\n",
       "      <td>29.58</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.568</td>\n",
       "      <td>41.382</td>\n",
       "      <td>325.8</td>\n",
       "      <td>38.0166</td>\n",
       "      <td>36.0</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>99.6</td>\n",
       "      <td>2347.50</td>\n",
       "      <td>68.28</td>\n",
       "      <td>33.98700</td>\n",
       "      <td>9000.0</td>\n",
       "      <td>1479.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>128.400</td>\n",
       "      <td>2069.100</td>\n",
       "      <td>16290.0</td>\n",
       "      <td>1900.8300</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>90000.0</td>\n",
       "      <td>4980.0</td>\n",
       "      <td>117375.00</td>\n",
       "      <td>3414.00</td>\n",
       "      <td>1.880614</td>\n",
       "      <td>498.0</td>\n",
       "      <td>81.838</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.1048</td>\n",
       "      <td>114.4902</td>\n",
       "      <td>901.38</td>\n",
       "      <td>105.17926</td>\n",
       "      <td>99.6</td>\n",
       "      <td>4980.0</td>\n",
       "      <td>275.56</td>\n",
       "      <td>6494.750</td>\n",
       "      <td>188.908</td>\n",
       "      <td>44.324713</td>\n",
       "      <td>11737.5</td>\n",
       "      <td>1928.8625</td>\n",
       "      <td>0.0</td>\n",
       "      <td>167.45500</td>\n",
       "      <td>2698.45125</td>\n",
       "      <td>21244.875</td>\n",
       "      <td>2478.999125</td>\n",
       "      <td>2347.50</td>\n",
       "      <td>117375.00</td>\n",
       "      <td>6494.750</td>\n",
       "      <td>153076.5625</td>\n",
       "      <td>4452.4250</td>\n",
       "      <td>1.289240</td>\n",
       "      <td>341.400</td>\n",
       "      <td>56.1034</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.87064</td>\n",
       "      <td>78.48786</td>\n",
       "      <td>617.934</td>\n",
       "      <td>72.104818</td>\n",
       "      <td>68.28</td>\n",
       "      <td>3414.00</td>\n",
       "      <td>188.908</td>\n",
       "      <td>4452.4250</td>\n",
       "      <td>129.5044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.08829</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>6.012</td>\n",
       "      <td>66.6</td>\n",
       "      <td>5.5605</td>\n",
       "      <td>5.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>395.60</td>\n",
       "      <td>12.43</td>\n",
       "      <td>0.007795</td>\n",
       "      <td>1.103625</td>\n",
       "      <td>0.694842</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.046264</td>\n",
       "      <td>0.530799</td>\n",
       "      <td>5.880114</td>\n",
       "      <td>0.490937</td>\n",
       "      <td>0.44145</td>\n",
       "      <td>27.45819</td>\n",
       "      <td>1.342008</td>\n",
       "      <td>34.927524</td>\n",
       "      <td>1.097445</td>\n",
       "      <td>1.103625</td>\n",
       "      <td>156.25</td>\n",
       "      <td>98.375</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.55</td>\n",
       "      <td>75.15</td>\n",
       "      <td>832.5</td>\n",
       "      <td>69.50625</td>\n",
       "      <td>62.5</td>\n",
       "      <td>3887.5</td>\n",
       "      <td>190.0</td>\n",
       "      <td>4945.0</td>\n",
       "      <td>155.375</td>\n",
       "      <td>0.694842</td>\n",
       "      <td>98.375</td>\n",
       "      <td>61.9369</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.12388</td>\n",
       "      <td>47.31444</td>\n",
       "      <td>524.142</td>\n",
       "      <td>43.761135</td>\n",
       "      <td>39.35</td>\n",
       "      <td>2447.57</td>\n",
       "      <td>119.624</td>\n",
       "      <td>3113.3720</td>\n",
       "      <td>97.8241</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.046264</td>\n",
       "      <td>6.55</td>\n",
       "      <td>4.12388</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.274576</td>\n",
       "      <td>3.150288</td>\n",
       "      <td>34.8984</td>\n",
       "      <td>2.913702</td>\n",
       "      <td>2.620</td>\n",
       "      <td>162.964</td>\n",
       "      <td>7.9648</td>\n",
       "      <td>207.29440</td>\n",
       "      <td>6.51332</td>\n",
       "      <td>0.530799</td>\n",
       "      <td>75.15</td>\n",
       "      <td>47.31444</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.150288</td>\n",
       "      <td>36.144144</td>\n",
       "      <td>400.3992</td>\n",
       "      <td>33.429726</td>\n",
       "      <td>30.060</td>\n",
       "      <td>1869.732</td>\n",
       "      <td>91.3824</td>\n",
       "      <td>2378.34720</td>\n",
       "      <td>74.72916</td>\n",
       "      <td>5.880114</td>\n",
       "      <td>832.5</td>\n",
       "      <td>524.142</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34.8984</td>\n",
       "      <td>400.3992</td>\n",
       "      <td>4435.56</td>\n",
       "      <td>370.32930</td>\n",
       "      <td>333.0</td>\n",
       "      <td>20712.6</td>\n",
       "      <td>1012.32</td>\n",
       "      <td>26346.960</td>\n",
       "      <td>827.838</td>\n",
       "      <td>0.490937</td>\n",
       "      <td>69.50625</td>\n",
       "      <td>43.761135</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2.913702</td>\n",
       "      <td>33.429726</td>\n",
       "      <td>370.32930</td>\n",
       "      <td>30.919160</td>\n",
       "      <td>27.8025</td>\n",
       "      <td>1729.3155</td>\n",
       "      <td>84.51960</td>\n",
       "      <td>2199.733800</td>\n",
       "      <td>69.117015</td>\n",
       "      <td>0.44145</td>\n",
       "      <td>62.5</td>\n",
       "      <td>39.35</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.620</td>\n",
       "      <td>30.060</td>\n",
       "      <td>333.0</td>\n",
       "      <td>27.8025</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1555.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>1978.00</td>\n",
       "      <td>62.15</td>\n",
       "      <td>27.45819</td>\n",
       "      <td>3887.5</td>\n",
       "      <td>2447.57</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162.964</td>\n",
       "      <td>1869.732</td>\n",
       "      <td>20712.6</td>\n",
       "      <td>1729.3155</td>\n",
       "      <td>1555.0</td>\n",
       "      <td>96721.0</td>\n",
       "      <td>4727.2</td>\n",
       "      <td>123031.60</td>\n",
       "      <td>3865.73</td>\n",
       "      <td>1.342008</td>\n",
       "      <td>190.0</td>\n",
       "      <td>119.624</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.9648</td>\n",
       "      <td>91.3824</td>\n",
       "      <td>1012.32</td>\n",
       "      <td>84.51960</td>\n",
       "      <td>76.0</td>\n",
       "      <td>4727.2</td>\n",
       "      <td>231.04</td>\n",
       "      <td>6013.120</td>\n",
       "      <td>188.936</td>\n",
       "      <td>34.927524</td>\n",
       "      <td>4945.0</td>\n",
       "      <td>3113.3720</td>\n",
       "      <td>0.0</td>\n",
       "      <td>207.29440</td>\n",
       "      <td>2378.34720</td>\n",
       "      <td>26346.960</td>\n",
       "      <td>2199.733800</td>\n",
       "      <td>1978.00</td>\n",
       "      <td>123031.60</td>\n",
       "      <td>6013.120</td>\n",
       "      <td>156499.3600</td>\n",
       "      <td>4917.3080</td>\n",
       "      <td>1.097445</td>\n",
       "      <td>155.375</td>\n",
       "      <td>97.8241</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.51332</td>\n",
       "      <td>74.72916</td>\n",
       "      <td>827.838</td>\n",
       "      <td>69.117015</td>\n",
       "      <td>62.15</td>\n",
       "      <td>3865.73</td>\n",
       "      <td>188.936</td>\n",
       "      <td>4917.3080</td>\n",
       "      <td>154.5049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>25.94060</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.679</td>\n",
       "      <td>5.304</td>\n",
       "      <td>89.1</td>\n",
       "      <td>1.6475</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>127.36</td>\n",
       "      <td>26.64</td>\n",
       "      <td>672.914728</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>469.524860</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>17.613667</td>\n",
       "      <td>137.588942</td>\n",
       "      <td>2311.307460</td>\n",
       "      <td>42.737139</td>\n",
       "      <td>622.57440</td>\n",
       "      <td>17276.43960</td>\n",
       "      <td>524.000120</td>\n",
       "      <td>3303.794816</td>\n",
       "      <td>691.057584</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>469.524860</td>\n",
       "      <td>0.000</td>\n",
       "      <td>327.6100</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.28990</td>\n",
       "      <td>96.00240</td>\n",
       "      <td>1612.710</td>\n",
       "      <td>29.819750</td>\n",
       "      <td>434.40</td>\n",
       "      <td>12054.60</td>\n",
       "      <td>365.620</td>\n",
       "      <td>2305.2160</td>\n",
       "      <td>482.1840</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>17.613667</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12.28990</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.461041</td>\n",
       "      <td>3.601416</td>\n",
       "      <td>60.4989</td>\n",
       "      <td>1.118653</td>\n",
       "      <td>16.296</td>\n",
       "      <td>452.214</td>\n",
       "      <td>13.7158</td>\n",
       "      <td>86.47744</td>\n",
       "      <td>18.08856</td>\n",
       "      <td>137.588942</td>\n",
       "      <td>0.00</td>\n",
       "      <td>96.00240</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.601416</td>\n",
       "      <td>28.132416</td>\n",
       "      <td>472.5864</td>\n",
       "      <td>8.738340</td>\n",
       "      <td>127.296</td>\n",
       "      <td>3532.464</td>\n",
       "      <td>107.1408</td>\n",
       "      <td>675.51744</td>\n",
       "      <td>141.29856</td>\n",
       "      <td>2311.307460</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1612.710</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.4989</td>\n",
       "      <td>472.5864</td>\n",
       "      <td>7938.81</td>\n",
       "      <td>146.79225</td>\n",
       "      <td>2138.4</td>\n",
       "      <td>59340.6</td>\n",
       "      <td>1799.82</td>\n",
       "      <td>11347.776</td>\n",
       "      <td>2373.624</td>\n",
       "      <td>42.737139</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>29.819750</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.118653</td>\n",
       "      <td>8.738340</td>\n",
       "      <td>146.79225</td>\n",
       "      <td>2.714256</td>\n",
       "      <td>39.5400</td>\n",
       "      <td>1097.2350</td>\n",
       "      <td>33.27950</td>\n",
       "      <td>209.825600</td>\n",
       "      <td>43.889400</td>\n",
       "      <td>622.57440</td>\n",
       "      <td>0.0</td>\n",
       "      <td>434.40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.296</td>\n",
       "      <td>127.296</td>\n",
       "      <td>2138.4</td>\n",
       "      <td>39.5400</td>\n",
       "      <td>576.0</td>\n",
       "      <td>15984.0</td>\n",
       "      <td>484.8</td>\n",
       "      <td>3056.64</td>\n",
       "      <td>639.36</td>\n",
       "      <td>17276.43960</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12054.60</td>\n",
       "      <td>0.0</td>\n",
       "      <td>452.214</td>\n",
       "      <td>3532.464</td>\n",
       "      <td>59340.6</td>\n",
       "      <td>1097.2350</td>\n",
       "      <td>15984.0</td>\n",
       "      <td>443556.0</td>\n",
       "      <td>13453.2</td>\n",
       "      <td>84821.76</td>\n",
       "      <td>17742.24</td>\n",
       "      <td>524.000120</td>\n",
       "      <td>0.0</td>\n",
       "      <td>365.620</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.7158</td>\n",
       "      <td>107.1408</td>\n",
       "      <td>1799.82</td>\n",
       "      <td>33.27950</td>\n",
       "      <td>484.8</td>\n",
       "      <td>13453.2</td>\n",
       "      <td>408.04</td>\n",
       "      <td>2572.672</td>\n",
       "      <td>538.128</td>\n",
       "      <td>3303.794816</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2305.2160</td>\n",
       "      <td>0.0</td>\n",
       "      <td>86.47744</td>\n",
       "      <td>675.51744</td>\n",
       "      <td>11347.776</td>\n",
       "      <td>209.825600</td>\n",
       "      <td>3056.64</td>\n",
       "      <td>84821.76</td>\n",
       "      <td>2572.672</td>\n",
       "      <td>16220.5696</td>\n",
       "      <td>3392.8704</td>\n",
       "      <td>691.057584</td>\n",
       "      <td>0.000</td>\n",
       "      <td>482.1840</td>\n",
       "      <td>0.00</td>\n",
       "      <td>18.08856</td>\n",
       "      <td>141.29856</td>\n",
       "      <td>2373.624</td>\n",
       "      <td>43.889400</td>\n",
       "      <td>639.36</td>\n",
       "      <td>17742.24</td>\n",
       "      <td>538.128</td>\n",
       "      <td>3392.8704</td>\n",
       "      <td>709.6896</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS   RAD    TAX  \\\n",
       "220   0.35809   0.0   6.20   1.0  0.507  6.951  88.5  2.8617   8.0  307.0   \n",
       "71    0.15876   0.0  10.81   0.0  0.413  5.961  17.5  5.2873   4.0  305.0   \n",
       "240   0.11329  30.0   4.93   0.0  0.428  6.897  54.3  6.3361   6.0  300.0   \n",
       "6     0.08829  12.5   7.87   0.0  0.524  6.012  66.6  5.5605   5.0  311.0   \n",
       "417  25.94060   0.0  18.10   0.0  0.679  5.304  89.1  1.6475  24.0  666.0   \n",
       "\n",
       "     PTRATIO       B  LSTAT   CRIMxCRIM   CRIMxZN  CRIMxINDUS  CRIMxCHAS  \\\n",
       "220     17.4  391.70   9.71    0.128228  0.000000    2.220158    0.35809   \n",
       "71      19.2  376.94   9.88    0.025205  0.000000    1.716196    0.00000   \n",
       "240     16.6  391.25  11.38    0.012835  3.398700    0.558520    0.00000   \n",
       "6       15.2  395.60  12.43    0.007795  1.103625    0.694842    0.00000   \n",
       "417     20.2  127.36  26.64  672.914728  0.000000  469.524860    0.00000   \n",
       "\n",
       "      CRIMxNOX     CRIMxRM     CRIMxAGE   CRIMxDIS   CRIMxRAD     CRIMxTAX  \\\n",
       "220   0.181552    2.489084    31.690965   1.024746    2.86472    109.93363   \n",
       "71    0.065568    0.946368     2.778300   0.839412    0.63504     48.42180   \n",
       "240   0.048488    0.781361     6.151647   0.717817    0.67974     33.98700   \n",
       "6     0.046264    0.530799     5.880114   0.490937    0.44145     27.45819   \n",
       "417  17.613667  137.588942  2311.307460  42.737139  622.57440  17276.43960   \n",
       "\n",
       "     CRIMxPTRATIO       CRIMxB  CRIMxLSTAT   ZNxCRIM   ZNxZN  ZNxINDUS  \\\n",
       "220      6.230766   140.263853    3.477054  0.000000    0.00     0.000   \n",
       "71       3.048192    59.842994    1.568549  0.000000    0.00     0.000   \n",
       "240      1.880614    44.324713    1.289240  3.398700  900.00   147.900   \n",
       "6        1.342008    34.927524    1.097445  1.103625  156.25    98.375   \n",
       "417    524.000120  3303.794816  691.057584  0.000000    0.00     0.000   \n",
       "\n",
       "     ZNxCHAS  ZNxNOX   ZNxRM  ZNxAGE     ZNxDIS  ZNxRAD  ZNxTAX  ZNxPTRATIO  \\\n",
       "220      0.0    0.00    0.00     0.0    0.00000     0.0     0.0         0.0   \n",
       "71       0.0    0.00    0.00     0.0    0.00000     0.0     0.0         0.0   \n",
       "240      0.0   12.84  206.91  1629.0  190.08300   180.0  9000.0       498.0   \n",
       "6        0.0    6.55   75.15   832.5   69.50625    62.5  3887.5       190.0   \n",
       "417      0.0    0.00    0.00     0.0    0.00000     0.0     0.0         0.0   \n",
       "\n",
       "        ZNxB  ZNxLSTAT  INDUSxCRIM  INDUSxZN  INDUSxINDUS  INDUSxCHAS  \\\n",
       "220      0.0     0.000    2.220158     0.000      38.4400         6.2   \n",
       "71       0.0     0.000    1.716196     0.000     116.8561         0.0   \n",
       "240  11737.5   341.400    0.558520   147.900      24.3049         0.0   \n",
       "6     4945.0   155.375    0.694842    98.375      61.9369         0.0   \n",
       "417      0.0     0.000  469.524860     0.000     327.6100         0.0   \n",
       "\n",
       "     INDUSxNOX  INDUSxRM  INDUSxAGE  INDUSxDIS  INDUSxRAD  INDUSxTAX  \\\n",
       "220    3.14340  43.09620    548.700  17.742540      49.60    1903.40   \n",
       "71     4.46453  64.43841    189.175  57.155713      43.24    3297.05   \n",
       "240    2.11004  34.00221    267.699  31.236973      29.58    1479.00   \n",
       "6      4.12388  47.31444    524.142  43.761135      39.35    2447.57   \n",
       "417   12.28990  96.00240   1612.710  29.819750     434.40   12054.60   \n",
       "\n",
       "     INDUSxPTRATIO    INDUSxB  INDUSxLSTAT  CHASxCRIM  CHASxZN  CHASxINDUS  \\\n",
       "220        107.880  2428.5400      60.2020    0.35809      0.0         6.2   \n",
       "71         207.552  4074.7214     106.8028    0.00000      0.0         0.0   \n",
       "240         81.838  1928.8625      56.1034    0.00000      0.0         0.0   \n",
       "6          119.624  3113.3720      97.8241    0.00000      0.0         0.0   \n",
       "417        365.620  2305.2160     482.1840    0.00000      0.0         0.0   \n",
       "\n",
       "     CHASxCHAS  CHASxNOX  CHASxRM  CHASxAGE  CHASxDIS  CHASxRAD  CHASxTAX  \\\n",
       "220        1.0     0.507    6.951      88.5    2.8617       8.0     307.0   \n",
       "71         0.0     0.000    0.000       0.0    0.0000       0.0       0.0   \n",
       "240        0.0     0.000    0.000       0.0    0.0000       0.0       0.0   \n",
       "6          0.0     0.000    0.000       0.0    0.0000       0.0       0.0   \n",
       "417        0.0     0.000    0.000       0.0    0.0000       0.0       0.0   \n",
       "\n",
       "     CHASxPTRATIO  CHASxB  CHASxLSTAT   NOXxCRIM  NOXxZN  NOXxINDUS  NOXxCHAS  \\\n",
       "220          17.4   391.7        9.71   0.181552    0.00    3.14340     0.507   \n",
       "71            0.0     0.0        0.00   0.065568    0.00    4.46453     0.000   \n",
       "240           0.0     0.0        0.00   0.048488   12.84    2.11004     0.000   \n",
       "6             0.0     0.0        0.00   0.046264    6.55    4.12388     0.000   \n",
       "417           0.0     0.0        0.00  17.613667    0.00   12.28990     0.000   \n",
       "\n",
       "      NOXxNOX    NOXxRM  NOXxAGE   NOXxDIS  NOXxRAD  NOXxTAX  NOXxPTRATIO  \\\n",
       "220  0.257049  3.524157  44.8695  1.450882    4.056  155.649       8.8218   \n",
       "71   0.170569  2.461893   7.2275  2.183655    1.652  125.965       7.9296   \n",
       "240  0.183184  2.951916  23.2404  2.711851    2.568  128.400       7.1048   \n",
       "6    0.274576  3.150288  34.8984  2.913702    2.620  162.964       7.9648   \n",
       "417  0.461041  3.601416  60.4989  1.118653   16.296  452.214      13.7158   \n",
       "\n",
       "         NOXxB  NOXxLSTAT     RMxCRIM   RMxZN  RMxINDUS  RMxCHAS    RMxNOX  \\\n",
       "220  198.59190    4.92297    2.489084    0.00  43.09620    6.951  3.524157   \n",
       "71   155.67622    4.08044    0.946368    0.00  64.43841    0.000  2.461893   \n",
       "240  167.45500    4.87064    0.781361  206.91  34.00221    0.000  2.951916   \n",
       "6    207.29440    6.51332    0.530799   75.15  47.31444    0.000  3.150288   \n",
       "417   86.47744   18.08856  137.588942    0.00  96.00240    0.000  3.601416   \n",
       "\n",
       "         RMxRM    RMxAGE     RMxDIS   RMxRAD    RMxTAX  RMxPTRATIO  \\\n",
       "220  48.316401  615.1635  19.891677   55.608  2133.957    120.9474   \n",
       "71   35.533521  104.3175  31.517595   23.844  1818.105    114.4512   \n",
       "240  47.568609  374.5071  43.700082   41.382  2069.100    114.4902   \n",
       "6    36.144144  400.3992  33.429726   30.060  1869.732     91.3824   \n",
       "417  28.132416  472.5864   8.738340  127.296  3532.464    107.1408   \n",
       "\n",
       "           RMxB   RMxLSTAT     AGExCRIM  AGExZN  AGExINDUS  AGExCHAS  AGExNOX  \\\n",
       "220  2722.70670   67.49421    31.690965     0.0    548.700      88.5  44.8695   \n",
       "71   2246.93934   58.89468     2.778300     0.0    189.175       0.0   7.2275   \n",
       "240  2698.45125   78.48786     6.151647  1629.0    267.699       0.0  23.2404   \n",
       "6    2378.34720   74.72916     5.880114   832.5    524.142       0.0  34.8984   \n",
       "417   675.51744  141.29856  2311.307460     0.0   1612.710       0.0  60.4989   \n",
       "\n",
       "       AGExRM  AGExAGE    AGExDIS  AGExRAD  AGExTAX  AGExPTRATIO      AGExB  \\\n",
       "220  615.1635  7832.25  253.26045    708.0  27169.5      1539.90  34665.450   \n",
       "71   104.3175   306.25   92.52775     70.0   5337.5       336.00   6596.450   \n",
       "240  374.5071  2948.49  344.05023    325.8  16290.0       901.38  21244.875   \n",
       "6    400.3992  4435.56  370.32930    333.0  20712.6      1012.32  26346.960   \n",
       "417  472.5864  7938.81  146.79225   2138.4  59340.6      1799.82  11347.776   \n",
       "\n",
       "     AGExLSTAT   DISxCRIM     DISxZN  DISxINDUS  DISxCHAS   DISxNOX  \\\n",
       "220    859.335   1.024746    0.00000  17.742540    2.8617  1.450882   \n",
       "71     172.900   0.839412    0.00000  57.155713    0.0000  2.183655   \n",
       "240    617.934   0.717817  190.08300  31.236973    0.0000  2.711851   \n",
       "6      827.838   0.490937   69.50625  43.761135    0.0000  2.913702   \n",
       "417   2373.624  42.737139    0.00000  29.819750    0.0000  1.118653   \n",
       "\n",
       "        DISxRM    DISxAGE    DISxDIS  DISxRAD    DISxTAX  DISxPTRATIO  \\\n",
       "220  19.891677  253.26045   8.189327  22.8936   878.5419     49.79358   \n",
       "71   31.517595   92.52775  27.955541  21.1492  1612.6265    101.51616   \n",
       "240  43.700082  344.05023  40.146163  38.0166  1900.8300    105.17926   \n",
       "6    33.429726  370.32930  30.919160  27.8025  1729.3155     84.51960   \n",
       "417   8.738340  146.79225   2.714256  39.5400  1097.2350     33.27950   \n",
       "\n",
       "           DISxB  DISxLSTAT   RADxCRIM  RADxZN  RADxINDUS  RADxCHAS  RADxNOX  \\\n",
       "220  1120.927890  27.787107    2.86472     0.0      49.60       8.0    4.056   \n",
       "71   1992.994862  52.238524    0.63504     0.0      43.24       0.0    1.652   \n",
       "240  2478.999125  72.104818    0.67974   180.0      29.58       0.0    2.568   \n",
       "6    2199.733800  69.117015    0.44145    62.5      39.35       0.0    2.620   \n",
       "417   209.825600  43.889400  622.57440     0.0     434.40       0.0   16.296   \n",
       "\n",
       "      RADxRM  RADxAGE  RADxDIS  RADxRAD  RADxTAX  RADxPTRATIO    RADxB  \\\n",
       "220   55.608    708.0  22.8936     64.0   2456.0        139.2  3133.60   \n",
       "71    23.844     70.0  21.1492     16.0   1220.0         76.8  1507.76   \n",
       "240   41.382    325.8  38.0166     36.0   1800.0         99.6  2347.50   \n",
       "6     30.060    333.0  27.8025     25.0   1555.0         76.0  1978.00   \n",
       "417  127.296   2138.4  39.5400    576.0  15984.0        484.8  3056.64   \n",
       "\n",
       "     RADxLSTAT     TAXxCRIM  TAXxZN  TAXxINDUS  TAXxCHAS  TAXxNOX    TAXxRM  \\\n",
       "220      77.68    109.93363     0.0    1903.40     307.0  155.649  2133.957   \n",
       "71       39.52     48.42180     0.0    3297.05       0.0  125.965  1818.105   \n",
       "240      68.28     33.98700  9000.0    1479.00       0.0  128.400  2069.100   \n",
       "6        62.15     27.45819  3887.5    2447.57       0.0  162.964  1869.732   \n",
       "417     639.36  17276.43960     0.0   12054.60       0.0  452.214  3532.464   \n",
       "\n",
       "     TAXxAGE    TAXxDIS  TAXxRAD   TAXxTAX  TAXxPTRATIO      TAXxB  TAXxLSTAT  \\\n",
       "220  27169.5   878.5419   2456.0   94249.0       5341.8  120251.90    2980.97   \n",
       "71    5337.5  1612.6265   1220.0   93025.0       5856.0  114966.70    3013.40   \n",
       "240  16290.0  1900.8300   1800.0   90000.0       4980.0  117375.00    3414.00   \n",
       "6    20712.6  1729.3155   1555.0   96721.0       4727.2  123031.60    3865.73   \n",
       "417  59340.6  1097.2350  15984.0  443556.0      13453.2   84821.76   17742.24   \n",
       "\n",
       "     PTRATIOxCRIM  PTRATIOxZN  PTRATIOxINDUS  PTRATIOxCHAS  PTRATIOxNOX  \\\n",
       "220      6.230766         0.0        107.880          17.4       8.8218   \n",
       "71       3.048192         0.0        207.552           0.0       7.9296   \n",
       "240      1.880614       498.0         81.838           0.0       7.1048   \n",
       "6        1.342008       190.0        119.624           0.0       7.9648   \n",
       "417    524.000120         0.0        365.620           0.0      13.7158   \n",
       "\n",
       "     PTRATIOxRM  PTRATIOxAGE  PTRATIOxDIS  PTRATIOxRAD  PTRATIOxTAX  \\\n",
       "220    120.9474      1539.90     49.79358        139.2       5341.8   \n",
       "71     114.4512       336.00    101.51616         76.8       5856.0   \n",
       "240    114.4902       901.38    105.17926         99.6       4980.0   \n",
       "6       91.3824      1012.32     84.51960         76.0       4727.2   \n",
       "417    107.1408      1799.82     33.27950        484.8      13453.2   \n",
       "\n",
       "     PTRATIOxPTRATIO  PTRATIOxB  PTRATIOxLSTAT       BxCRIM     BxZN  \\\n",
       "220           302.76   6815.580        168.954   140.263853      0.0   \n",
       "71            368.64   7237.248        189.696    59.842994      0.0   \n",
       "240           275.56   6494.750        188.908    44.324713  11737.5   \n",
       "6             231.04   6013.120        188.936    34.927524   4945.0   \n",
       "417           408.04   2572.672        538.128  3303.794816      0.0   \n",
       "\n",
       "       BxINDUS  BxCHAS      BxNOX        BxRM      BxAGE        BxDIS  \\\n",
       "220  2428.5400   391.7  198.59190  2722.70670  34665.450  1120.927890   \n",
       "71   4074.7214     0.0  155.67622  2246.93934   6596.450  1992.994862   \n",
       "240  1928.8625     0.0  167.45500  2698.45125  21244.875  2478.999125   \n",
       "6    3113.3720     0.0  207.29440  2378.34720  26346.960  2199.733800   \n",
       "417  2305.2160     0.0   86.47744   675.51744  11347.776   209.825600   \n",
       "\n",
       "       BxRAD      BxTAX  BxPTRATIO          BxB    BxLSTAT  LSTATxCRIM  \\\n",
       "220  3133.60  120251.90   6815.580  153428.8900  3803.4070    3.477054   \n",
       "71   1507.76  114966.70   7237.248  142083.7636  3724.1672    1.568549   \n",
       "240  2347.50  117375.00   6494.750  153076.5625  4452.4250    1.289240   \n",
       "6    1978.00  123031.60   6013.120  156499.3600  4917.3080    1.097445   \n",
       "417  3056.64   84821.76   2572.672   16220.5696  3392.8704  691.057584   \n",
       "\n",
       "     LSTATxZN  LSTATxINDUS  LSTATxCHAS  LSTATxNOX   LSTATxRM  LSTATxAGE  \\\n",
       "220     0.000      60.2020        9.71    4.92297   67.49421    859.335   \n",
       "71      0.000     106.8028        0.00    4.08044   58.89468    172.900   \n",
       "240   341.400      56.1034        0.00    4.87064   78.48786    617.934   \n",
       "6     155.375      97.8241        0.00    6.51332   74.72916    827.838   \n",
       "417     0.000     482.1840        0.00   18.08856  141.29856   2373.624   \n",
       "\n",
       "     LSTATxDIS  LSTATxRAD  LSTATxTAX  LSTATxPTRATIO    LSTATxB  LSTATxLSTAT  \n",
       "220  27.787107      77.68    2980.97        168.954  3803.4070      94.2841  \n",
       "71   52.238524      39.52    3013.40        189.696  3724.1672      97.6144  \n",
       "240  72.104818      68.28    3414.00        188.908  4452.4250     129.5044  \n",
       "6    69.117015      62.15    3865.73        188.936  4917.3080     154.5049  \n",
       "417  43.889400     639.36   17742.24        538.128  3392.8704     709.6896  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extended_x_train.head()\n",
    "#columns의 수는 (원래 칼럼수 * 월래 칼럼수) + 원래 칼럼수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9b0fc7de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404, 182)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extended_x_train.shape\n",
    "#형태 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecea2dfe",
   "metadata": {},
   "source": [
    "#### 특성 확장된 train 데이터의 값과 정답으로 모델을 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f1cd5e1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr2 = LinearRegression()\n",
    "lr2.fit(extended_x_train,y_train)\n",
    "#모델 선언 및 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1aff6c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_x_test = x_test.copy()\n",
    "#x_test 데이터 복제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fabba27b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\793332256.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1+'x'+col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\793332256.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1+'x'+col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\793332256.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1+'x'+col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\793332256.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1+'x'+col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\793332256.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1+'x'+col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\793332256.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1+'x'+col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\793332256.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1+'x'+col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\793332256.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1+'x'+col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\793332256.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1+'x'+col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\793332256.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1+'x'+col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\793332256.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1+'x'+col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\793332256.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1+'x'+col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\793332256.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1+'x'+col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\793332256.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1+'x'+col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\793332256.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1+'x'+col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\793332256.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1+'x'+col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\793332256.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1+'x'+col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\793332256.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1+'x'+col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\793332256.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1+'x'+col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\793332256.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1+'x'+col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\793332256.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1+'x'+col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\793332256.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1+'x'+col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\793332256.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1+'x'+col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\793332256.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1+'x'+col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\793332256.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1+'x'+col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\793332256.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1+'x'+col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\793332256.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1+'x'+col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\793332256.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1+'x'+col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\793332256.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1+'x'+col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\793332256.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1+'x'+col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\793332256.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1+'x'+col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\793332256.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1+'x'+col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\793332256.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1+'x'+col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\793332256.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1+'x'+col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\793332256.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1+'x'+col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\793332256.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1+'x'+col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\793332256.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1+'x'+col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\793332256.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1+'x'+col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\793332256.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1+'x'+col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\793332256.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1+'x'+col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\793332256.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1+'x'+col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\793332256.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1+'x'+col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\793332256.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1+'x'+col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\793332256.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1+'x'+col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\793332256.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1+'x'+col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\793332256.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1+'x'+col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\793332256.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1+'x'+col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\793332256.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1+'x'+col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\793332256.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1+'x'+col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\793332256.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1+'x'+col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\793332256.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1+'x'+col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\793332256.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1+'x'+col2] = x_test[col1] * x_test[col2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\793332256.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1+'x'+col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\793332256.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1+'x'+col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\793332256.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1+'x'+col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\793332256.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1+'x'+col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\793332256.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1+'x'+col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\793332256.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1+'x'+col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\793332256.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1+'x'+col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\793332256.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1+'x'+col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\793332256.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1+'x'+col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\793332256.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1+'x'+col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\793332256.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1+'x'+col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\793332256.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1+'x'+col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\793332256.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1+'x'+col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\793332256.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1+'x'+col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\793332256.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1+'x'+col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\793332256.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1+'x'+col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\793332256.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1+'x'+col2] = x_test[col1] * x_test[col2]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12264\\793332256.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  extended_x_test[col1+'x'+col2] = x_test[col1] * x_test[col2]\n"
     ]
    }
   ],
   "source": [
    "#extended_x_train의 각 컬럼들을 서로 한번씩 곱해서 새로운 컬럼 추가.\n",
    "#x_test데이터 특성확장 코드\n",
    "for col1 in x_test.columns: #x_train과 x_test의 column은 같다.\n",
    "    for col2 in x_test.columns:\n",
    "        \n",
    "        extended_x_test[col1+'x'+col2] = x_test[col1] * x_test[col2]\n",
    "        #x_test로 쓰지 않으면 오류 발생"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6b661a97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102, 182)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extended_x_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e43b575",
   "metadata": {},
   "source": [
    "#### 오차 = 예측 - 정답"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5653f3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_pre_train = lr2.predict(extended_x_train)\n",
    "ex_pre_test = lr2.predict(extended_x_test)\n",
    "#결과 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "83d5ae8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_train_mse = mean_squared_error(ex_pre_train,y_train)\n",
    "ex_test_mse = mean_squared_error(ex_pre_test,y_test)\n",
    "#mse 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d159d4c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train mse: 4.340278052012249\n"
     ]
    }
   ],
   "source": [
    "print('train mse:',ex_train_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "83bd7fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test mse: 31.277814972583755\n"
     ]
    }
   ],
   "source": [
    "print('test mse:',ex_test_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "68a431a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train,test mse의 차이: -26.937536920571507\n"
     ]
    }
   ],
   "source": [
    "print('train,test mse의 차이:',ex_train_mse - ex_test_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4595edcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train rmse: 2.0833333991496055\n"
     ]
    }
   ],
   "source": [
    "print('train rmse:',ex_train_mse ** 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "214a78fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test rmse: 5.592657237180172\n"
     ]
    }
   ],
   "source": [
    "print('test rmse:',ex_test_mse ** 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d331b8c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train , test rmse 차이: 3.509323838030567\n"
     ]
    }
   ],
   "source": [
    "print('train , test rmse 차이:',ex_test_mse**0.5 - ex_train_mse**0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2c742eb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9490240966612834"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr2.score(extended_x_train,y_train)\n",
    "#특성확장 훈련 데이터셋 r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5a6dc0de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6158858583939284"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr2.score(extended_x_test,y_test)\n",
    "#특성확장 테스트 데이터셋 r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d45325e",
   "metadata": {},
   "source": [
    "훈련데이터에만 설명력이 높으니 과대적합의 가능성이 있다.\n",
    "    -- 과대적합이라고 단언할 수 없는 이유는 분야가 다르기 때문"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd045df9",
   "metadata": {},
   "source": [
    "### Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "dfd01fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7be5f2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = Lasso(alpha=10)\n",
    "#규제의 강도 α = 10 , α값이 높아지면 과대적합이 줄어듬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "75b81f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.194e+03, tolerance: 3.440e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Lasso(alpha=10)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso.fit(extended_x_train,y_train)\n",
    "#모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "99e99a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사용한 특성의 개수: 46\n"
     ]
    }
   ],
   "source": [
    "print('사용한 특성의 개수:', np.sum(lasso.coef_ !=0 ))\n",
    "#가중치가 0이 아닌 특성들의 수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "777760a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_train_lasso = lasso.predict(extended_x_train)\n",
    "pre_test_lasso = lasso.predict(extended_x_test)\n",
    "#예측값 산출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "871e85f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_train_mse = mean_squared_error(pre_train_lasso , y_train)\n",
    "lasso_test_mse = mean_squared_error(pre_test_lasso, y_test)\n",
    "#MSE 산출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "80c24c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train mse: 9.825814145224081\n",
      "test mse: 26.404674056715386\n",
      "train,test mse 차이: 16.578859911491307\n"
     ]
    }
   ],
   "source": [
    "print('train mse:',lasso_train_mse)\n",
    "print('test mse:',lasso_test_mse)\n",
    "print('train,test mse 차이:',abs(lasso_train_mse-lasso_test_mse)) #오차"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "19638618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train rmse: 3.1346154700734954\n",
      "test rmse: 5.138547854862829\n",
      "train,test rmse 차이: 2.003932384789334\n"
     ]
    }
   ],
   "source": [
    "print('train rmse:',lasso_train_mse ** 0.5)\n",
    "print('test rmse:',lasso_test_mse ** 0.5)\n",
    "print('train,test rmse 차이:',abs(lasso_train_mse **0.5 - lasso_test_mse**0.5)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6f947dab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8845973124097618"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso.score(extended_x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "68dfe5c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6757315458712968"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso.score(extended_x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc20cb4",
   "metadata": {},
   "source": [
    "### Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6ede66ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2f1e91b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = Ridge(alpha=10)\n",
    "#규제의 강도 α = 10 , α값이 높아지면 과대적합이 줄어듬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4c04037c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ridge(alpha=10)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge.fit(extended_x_train,y_train)\n",
    "#모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9e0e168c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_train_ridge = ridge.predict(extended_x_train)\n",
    "pre_test_ridge = ridge.predict(extended_x_test)\n",
    "#예측값 산출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f2baf836",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_train_mse=mean_squared_error(pre_train_ridge,y_train)\n",
    "ridge_test_mse = mean_squared_error(pre_test_ridge,y_test)\n",
    "#MSE 산출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "64e6c0fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train mse: 5.09456324639931\n",
      "test mse: 26.79259651669547\n",
      "train, test mse 차이: 21.698033270296158\n"
     ]
    }
   ],
   "source": [
    "print('train mse:',ridge_train_mse)\n",
    "print('test mse:',ridge_test_mse)\n",
    "print('train, test mse 차이:',abs(ridge_train_mse-ridge_test_mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c56eb59e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train rmse: 2.25711391967692\n",
      "test rmse: 5.176156539044725\n",
      "train, test rmse 차이: 2.919042619367805\n"
     ]
    }
   ],
   "source": [
    "print('train rmse:',ridge_train_mse ** 0.5)\n",
    "print('test rmse:',ridge_test_mse ** 0.5)\n",
    "print('train, test rmse 차이:',abs(ridge_train_mse**0.5-ridge_test_mse**0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "74894049",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9401651321668143"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge.score(extended_x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2bc53f93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6709675780923591"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge.score(extended_x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9471cbeb",
   "metadata": {},
   "source": [
    " 같은 alpha 값을 가질 때, Ridge 모델보다 Lasso 모델에서 과대적합을 잘 해소한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9381ed88",
   "metadata": {},
   "source": [
    "### 하이퍼파라미터 튜닝\n",
    "- alpha 값을 바꿔가면서 rmse 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc976e47",
   "metadata": {},
   "source": [
    "#### 라쏘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "800b341a",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_list= [0.001,0.01,0.1,1,10,100,1000]\n",
    "lasso_train_list=[] #train데이터의 rmse값을 담아줄 빈 리스트\n",
    "lasso_test_list=[] #test데이터의 rmse값을 담아줄 빈 리스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "fc131355",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.071e+03, tolerance: 3.440e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.188e+03, tolerance: 3.440e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.462e+03, tolerance: 3.440e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.803e+03, tolerance: 3.440e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.194e+03, tolerance: 3.440e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.870e+01, tolerance: 3.440e+00\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "#alpha의 값을 바꿔가며 반복하기 위한 for문\n",
    "for i in alpha_list:\n",
    "    #모델 선정 및 학습\n",
    "    lasso = Lasso(alpha = i)\n",
    "    lasso.fit(extended_x_train,y_train)\n",
    "    \n",
    "    #train데이터 예측값, rmse 산출\n",
    "    train_pred =lasso.predict(extended_x_train)\n",
    "    lasso_train_rmse = mean_squared_error(train_pred,y_train)**0.5\n",
    "    lasso_train_list.append(lasso_train_rmse)\n",
    "    \n",
    "    #test데이터 예측값, rmse 산출\n",
    "    test_pred =lasso.predict(extended_x_test)\n",
    "    lasso_test_rmse = mean_squared_error(test_pred,y_test)**0.5\n",
    "    lasso_test_list.append(lasso_test_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ffbf0175",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.2758518221817816,\n",
       " 2.332152582439725,\n",
       " 2.556464318296377,\n",
       " 2.8421497511641136,\n",
       " 3.1346154700734954,\n",
       " 3.9961466355356023,\n",
       " 5.353949685680336]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso_train_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b8d87f12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5.795351938140569,\n",
       " 5.447613887368501,\n",
       " 4.676403047411522,\n",
       " 5.053153764089218,\n",
       " 5.138547854862829,\n",
       " 5.737963242918281,\n",
       " 6.985827230756549]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso_test_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "93fbe622",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8e644cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rc('font', family='Malgun Gothic')\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "#글꼴설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "15954fed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(lasso_train_list, label='train')\n",
    "plt.plot(lasso_test_list,label='test')\n",
    "plt.title('Lasso 모델 alpha - RMSE 그래프')\n",
    "plt.xlabel=('alpha 값')\n",
    "plt.xticks(np.arange(7),alpha_list) #x축 범위 설정(x축 값의 개수,어떻게 바꿔줄지)\n",
    "plt.ylabel=('RMSE')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc342e1",
   "metadata": {},
   "source": [
    "### 릿지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "fc6c2af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_list= [0.001,0.01,0.1,1,10,100,1000]\n",
    "ridge_train_list=[] #train데이터의 rmse값을 담아줄 빈 리스트\n",
    "ridge_test_list=[] #test데이터의 rmse값을 담아줄 빈 리스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "421122bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=4.05867e-17): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T\n"
     ]
    }
   ],
   "source": [
    "#alpha의 값을 바꿔가며 반복하기 위한 for문\n",
    "for i in alpha_list:\n",
    "    #모델 선정 및 학습\n",
    "    ridge = Ridge(alpha = i)\n",
    "    ridge.fit(extended_x_train,y_train)\n",
    "    \n",
    "    #train데이터 예측값, rmse 산출\n",
    "    train_pred =ridge.predict(extended_x_train)\n",
    "    ridge_train_rmse = mean_squared_error(train_pred,y_train)**0.5\n",
    "    ridge_train_list.append(ridge_train_rmse)\n",
    "    \n",
    "    #test데이터 예측값, rmse 산출\n",
    "    test_pred =ridge.predict(extended_x_test)\n",
    "    ridge_test_rmse = mean_squared_error(test_pred,y_test)**0.5\n",
    "    ridge_test_list.append(ridge_test_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "1ee0a063",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.0835065507717947,\n",
       " 2.0850764409779257,\n",
       " 2.0994432636573346,\n",
       " 2.1580820650757446,\n",
       " 2.25711391967692,\n",
       " 2.4260271435858827,\n",
       " 2.599646390917805]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge_train_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "84e22bb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5.653502668471952,\n",
       " 5.874871100553151,\n",
       " 6.0103239800468335,\n",
       " 5.783909816750169,\n",
       " 5.176156539044725,\n",
       " 4.765046455866838,\n",
       " 4.8818074802533555]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge_test_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "74fc370a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(ridge_train_list, label='train')\n",
    "plt.plot(ridge_test_list,label='test')\n",
    "plt.title('Ridge 모델 alpha - RMSE 그래프')\n",
    "plt.xlabel=('alpha 값')\n",
    "plt.xticks(np.arange(7),alpha_list) #x축 범위 설정(x축 값의 개수,어떻게 바꿔줄지)\n",
    "plt.ylabel=('RMSE')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e521e27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
